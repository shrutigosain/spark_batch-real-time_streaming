{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa07d4a9-ddc5-482a-8050-1245cd27e476",
   "metadata": {},
   "source": [
    "### Read from Website_stats and publish to Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a5f2793-9089-4d99-9b48-205e368ae589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying bounds for date ranges : 2025-04-03 2025-04-04\n",
      "Query bounds are :  1 38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/03 21:06:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/03 21:06:48 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/04/03 21:06:48 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+--------+\n",
      "| ID| LAST_ACTION|DURATION|\n",
      "+---+------------+--------+\n",
      "|  3|ShoppingCart|      17|\n",
      "|  5|ShoppingCart|      20|\n",
      "|  6|         FAQ|      22|\n",
      "|  7|ShoppingCart|      19|\n",
      "| 12|         FAQ|      26|\n",
      "| 21|ShoppingCart|      16|\n",
      "| 23|     Catalog|      20|\n",
      "| 28|ShoppingCart|      19|\n",
      "| 30|       Order|      23|\n",
      "| 32|       Order|      16|\n",
      "+---+------------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import mariadb\n",
    "import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#Connect to website_stats database\n",
    "summary_conn = mariadb.connect(\n",
    "                user=\"spark\",\n",
    "                password=\"spark\",\n",
    "                host=\"127.0.0.1\",\n",
    "                port=3306,\n",
    "                database=\"website_stats\",\n",
    "                autocommit=True\n",
    "            )\n",
    "summary_cursor = summary_conn.cursor()\n",
    "\n",
    "#This assumes that the pipeline is executed on the same day as when\n",
    "#the website_stats db is populated\n",
    "start_date=datetime.datetime.today().strftime(\"%Y-%m-%d\")\n",
    "end_date=(datetime.datetime.today()+datetime.timedelta(1)).strftime(\"%Y-%m-%d\")\n",
    "print(\"Querying bounds for date ranges :\", start_date,end_date)\n",
    "\n",
    "\n",
    "#find min and max bounds for the parallel DB query\n",
    "summary_cursor.execute(f\"\"\"\n",
    "        SELECT min(`ID`) as MIN_ID, max(`ID`) as MAX_ID \n",
    "        FROM `website_stats`.`visit_stats`\n",
    "        WHERE `INTERVAL_TIMESTAMP` BETWEEN \n",
    "            '{start_date}' AND '{end_date}'\n",
    "    \"\"\")\n",
    "\n",
    "min_bounds=0\n",
    "max_bounds=0\n",
    "for min_id, max_id in summary_cursor:\n",
    "    min_bounds=min_id\n",
    "    max_bounds=max_id\n",
    "print(\"Query bounds are : \", min_id, max_id)\n",
    "\n",
    "#Get all last_actions where duration > 15 seconds\n",
    "last_action_query= f\"\"\"\n",
    "            SELECT ID, LAST_ACTION, DURATION\n",
    "            FROM `website_stats`.`visit_stats`\n",
    "             WHERE `INTERVAL_TIMESTAMP` BETWEEN \n",
    "                '{start_date}' AND '{end_date}'     \n",
    "            AND DURATION > 15\n",
    "        \"\"\"\n",
    "\n",
    "#create spark session\n",
    "website_spark = SparkSession\\\n",
    "            .builder\\\n",
    "            .appName(\"LongLastActionsJob\")\\\n",
    "            .config(\"spark.sql.shuffle.partitions\", 2)\\\n",
    "            .config(\"spark.default.parallelism\", 2)\\\n",
    "            .config(\"spark.sql.streaming.forceDeleteTempCheckpointLocation\", True)\\\n",
    "            .config(\"spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version\",\"2\")\\\n",
    "            .config(\"spark.jars\", \"jars/mysql-connector-j-8.4.0.jar,\" +\\\n",
    "                                    \"jars/commons-pool2-2.12.0.jar,\" +\\\n",
    "                                    \"jars/kafka-clients-3.6.0.jar,\" + \\\n",
    "                                    \"jars/spark-sql-kafka-0-10_2.12-3.5.1.jar,\" +\\\n",
    "                                    \"jars/spark-token-provider-kafka-0-10_2.12-3.5.1.jar,\" +\\\n",
    "                                    \"jars/spark-streaming-kafka-0-10_2.12-3.5.1.jar\") \\\n",
    "            .config(\"spark.driver.extraClassPath\",\"jars/*\") \\\n",
    "            .master(\"local[2]\")\\\n",
    "            .getOrCreate()\n",
    "\n",
    "#Read long last actions with distributed processing\n",
    "last_action_df = website_spark.read\\\n",
    "            .format(\"jdbc\")\\\n",
    "            .option(\"url\", \"jdbc:mysql://localhost:3306/website_stats\")\\\n",
    "            .option(\"dbtable\", \"( \" + last_action_query + \" ) as tmpLastAction\")\\\n",
    "            .option(\"user\", \"spark\")\\\n",
    "            .option(\"password\", \"spark\")\\\n",
    "            .option(\"partitionColumn\",\"ID\")\\\n",
    "            .option(\"lowerBound\", min_bounds)\\\n",
    "            .option(\"upperBound\",max_bounds + 1)\\\n",
    "            .option(\"numPartitions\",2)\\\n",
    "            .load()\n",
    "    \n",
    "last_action_df.show(10)\n",
    "\n",
    "#Send the last actions to the Kafka topic\n",
    "last_action_df.selectExpr(\"LAST_ACTION as key\", \"LAST_ACTION as value\")\\\n",
    "        .write\\\n",
    "        .format(\"kafka\")\\\n",
    "        .option(\"checkpointLocation\", \"/tmp/cp-lastaction\")\\\n",
    "        .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\\\n",
    "        .option(\"topic\", \"spark.exercise.lastaction.long\")\\\n",
    "        .save();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d677edfc-2543-4d3c-b357-f0b5cddc0285",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
